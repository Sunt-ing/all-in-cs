来源：https://zhuanlan.zhihu.com/p/104444471



个人感觉 MLsys 不能算是一种方向，而是一种思路...... 比如对于 system 研究者来说，可以把 ML 作为我们开发的系统要适配的一种 benchmark，就像 transaction 对于数据库、某种文件场景对于 File System 的意义一样。就算 ML 哪天又进入寒冬，之前所学的技术也仍然是可持续的。传统的 system 研究者也应该适应这个潮流，不能简单的把 MLsys 一律归为大水漫灌..



#### **1. 分布式机器学习（Distributed DNN Training）**

这个又可以分为两个方面：from ML /system perspective。安利一下刘铁岩老师的[《分布式机器学习》](https://link.zhihu.com/?target=https%3A//detail.tmall.com/item.htm%3Fspm%3Da230r.1.14.16.92fb3db98MysR9%26id%3D579069032926%26ns%3D1%26abbucket%3D6)这本书（[ch_] 表示引用这本书中的一些章节），还有 UCB cs294 19fall 的[这一节](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec06/06_distributed_training.pdf)。

##### **ML**

从 ML 的角度做，主要是发明或改进分布式训练算法 [ch4] [ch5]，保证在分布式加速的同时，仍然能达到原来的学习效果（loss/accuracy）。因此很多工作也被投在像 ICML、NIPS 这种专业 ML 会议上。主要用到的方法包括优化（optimization）和统计学习理论（statistical learning theory）。

还有一类工作涉及到如何把单机算法改造成分布式 [ch9]，比如同步 / 异步 SGD 等。这里主要涉及到的问题是如何降低分布式环境下的通信开销，提高加速比。

可以参考[这里](https://zhuanlan.zhihu.com/p/29032307)。

##### **System**

还有一个就是从 System 的角度做。从分布式计算的角度来看，可以把相关工作分为[以下几类](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pdev/p/11528359.html)：

**计算并行**：对于计算量太大的场景，可以多线程 / 多节点并行计算，多节点共享公共的存储空间。常用的一个算法就是同步随机梯度下降（synchronous stochastic gradient descent），含义大致相当于 K 个（K 是节点数）mini-batch SGD [ch6.2]

**数据并行**：**是最主要的场景**，训练数据太多，单机放不下，需要将数据划分到多个节点上训练。每个节点先用本地的数据先训练出一个子模型，同时和其他节点保持通信（比如更新参数）以保证最终可以有效整合来自各个节点的训练结果，并得到全局的 ML 模型。 [ch6.3]

**模型并行**：模型太大，需要把模型（例如 NN 中的不同层）划分到不同节点上进行训练。此时不同节点之间可能需要频繁的 sync。[ch6.4]

**Pipeline Parallelism**：这是去年（SOSP19 PipeDream）才出现的概念，参考[这里的](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec06/06_distributed_training.pdf)第 90、95 页 以及[这里的简介](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/IQbBD_RxbeecrlPUiKcLjQ)。Pipeline Parallelism 相当于把数据并行和模型并行结合起来，把数据划分成多个 chunk，也把训练模型的过程分成了 Forward Pass 和 Backward Pass 两个 stage。然后用流水线的思想进行计算。



另外，分布式 ML 本质上还是分布式系统嘛，所以像传统分布式系统里的一些 topic（比如一致性、fault tolerance、通信、load balance 等等）也可以放到这个背景下进行研究。

##### **1.1. 分布式 ML 系统设计**

[ch7.3] 最著名的就是几大分布式 DL 模型：[Parameter Server](https://zhuanlan.zhihu.com/p/30976469) / AllReduce 等。

个人感觉这里面一个可以挖的坑是 Decentralized Training。地里一位[大佬](https://link.zhihu.com/?target=https%3A//chaoyanghe.com/)也在做这个方向。

##### **1.2 Edge Computing**

很多 ML 模型是需要在手机上运行的（比如毁图秀秀）。针对这一场景，一个是要对手机这种低功耗设备对 ML model 进行裁剪加速（后面会提到），还有一个要做的就是运行在多个 device 上的分布式 ML。

这里有个最近非常火的概念：[Federated Learning](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pdev/p/11655467.html)。其实本质还是炒数据并行的冷饭... 不过应用场景比较不一样。FL 更多是为了 Privacy 的考虑，而分布式加速训练在这里倒是个次要目标。FL 还涉及到了模型聚合 [ch8]，也就是如何把多个 device 本地训练出的模型合并到一起。

##### **1.3 大量计算资源的 Scheduling /device placement**

UCB 的 CS294 19spring 对[这一节](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-sp19/assets/lectures/lec27/dl-scheduling.pdf)有过介绍。

这里的计算资源的数量级是很大的...... 比如工业界会有万台 CPU 服务器 / 上千台 GPU 服务器搭建的 DL 平台。这个小方向要解决的问题就是如何充分利用它们的性能。比如在[阿里 PAI 组的 JD](https://zhuanlan.zhihu.com/p/36316064) 里就有这么一条：“设计探索高效的分布式 Placement 算法，以更系统化的方式来解决大规模深度学习高效训练的问题”。

这方面比较早的工作大概是[这篇 paper](https://link.zhihu.com/?target=http%3A//www-users.cselabs.umn.edu/classes/Spring-2019/csci8980/papers/device_placement.pdf)，说的是如何为 TensorFlow 计算图里的不同算子分配不同的 device，最后用强化学习实现了这个目标。这个工作看起来有点 prototype，但提出了一个新的思路。另外还有很多猛如虎的类似 [Train XX model in y minutes](https://www.zhihu.com/question/60874090) 的工作。这种就不仅是 placement 好就能完成的了，还需要涉及系统拓扑的设计、降低 communication 开销等等。

对于集群调度，工业界的一个热点是使用容器平台（例如 [k8s](https://zhuanlan.zhihu.com/p/29691157)）来运行分布式机器学习应用。虽然 k8s 本身就有容器集群调度的功能，但为了让它更好地适应 ML 的 workload，人们开发了一些新的轮子，比如针对 TensorFlow（Parameter Server 模型）和 [PyTorch](https://link.zhihu.com/?target=https%3A//github.com/kubeflow/pytorch-operator) 的 [KubeFlow](https://link.zhihu.com/?target=https%3A//www.kubeflow.org/)。还有用 k8s 来跑 AutoML 的 [katib](https://zhuanlan.zhihu.com/p/77760872)。学术界对这方面的一个研究热点是 GPU 集群调度，在下面 2.2 节会介绍。

##### **1.4 communication 相关**

[ch3.5] [ch7] 介绍了一些宏观上的通信模型，但深入进去还有很多可搞的坑。传统搞网络 / 分布式系统的组比较契合这个小方向。

例如我校的分布式组原来有一些 [geo-distributed system](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pdev/p/11655467.html) 的工作，现在也可以往 ML 上装。

##### **1.5 其他 sys for ML 可做的坑**

工业界的一个 ML pipeline 不仅仅是训练，还涉及到很多其他的坑。这些是目前被挖的还比较少的：

- 存储 / Data Management：

- - 1) 训练数据的规模是很大的。如何为 ML 设计一个专用的文件系统（类似大数据界的 HDFS）或者数据库来加速读数据呢？ 类似的工作有管理 ML model 的 [ModelDB](https://link.zhihu.com/?target=https%3A//github.com/mitdbg/modeldb).
  - 2) 在 ML framework 中，以及 Parameter Server 中，需要用一个 KV storage system 来存储参数。可不可以针对 ML 的场景优化这个 KV 存储系统呢？ 关于这个可以参考 [neopenx 大神的 blog](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/neopenx/p/5269852.html)。

#### **2. 深度学习模型压缩 / 加速**

这方面和 architecture 结合比较紧密。CS229 有[这一节](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-sp19/assets/lectures/lec22/network_compression.pdf)，也可以参考 [NIPS19 上的这个 talk](https://link.zhihu.com/?target=http%3A//eyeriss.mit.edu/2019_neurips_tutorial.pdf%3Fich_args2%3D526-06113205060278_e0b61138dc2d908aa0766d50302e6d8a_10001002_9c896324d5cbf0d49239518939a83798_92e0a859ed6aa28355c3deea32562911)。

对 DL model 进行压缩主要考虑两个角度：减少计算量（例如 conv 层的计算量） / 内存占用（NN 的参数数量）。不仅要考虑 ML 上的 metric，也要考虑 system 层面的 performance（例如 latency /throughput/ 功耗。有时候这些比 ML 模型的 accuracy 还重要）。具体的方式大概有[以下几种](https://zhuanlan.zhihu.com/p/101544149)：

- Architectural Compression

- - Layer Design -> Typically using factorization techniques to reduce storage and computation
  - Pruning（剪枝） -> Eliminating weights, layers, or channels to reduce storage and computation from large pre-trained models. 减少卷积核大小 / 通道数等等

- Weight Compression

- - Low Bit Precision Arithmetic -> Weights and activations are stored and computed using low bit precision
  - Quantized（量化） Weight Encoding -> Weights are quantized and stored using dictionary encodings.

很多相关的工作是在 ML 的角度来压缩模型的（也就是 Arch Compression，特别是针对 CNN 和 RNN。比如很著名的 MobileNet）。这里我们先 (kan) 略 (bu) 过 (dong)，来看从 System 的角度是如何加速的。

##### **2.1 通过 Quantized（量化）降低计算精度要求**

量化的含义是将卷积层（the weights and /or activations of a CNN）通常要用到的 32 位浮点数用更低位的数来表示，如 int32, int16, int8 等等，来降低资源占用（float32 无论是计算还是存储都是很吃资源的..）。量化之后无疑会损失一部分精度，但神经网络对噪声并不是特别敏感，因此控制好量化的程度之后对 ML 任务的影响可以很小。

一种常用的量化方法是 train in floating point and then quantize the resulting weights，训练时还是用 float32（因为要涉及到反向传播和梯度下降，全是 int 就很难搞了..），但在 inference 的阶段就可以加速啦。一个直观的方法是事先找好一般网络参数的 min /max 值，然后将训练好的网络参数乘一个 scala factor 来映射到 [MIN_INT, MAX_INT] 区间内的整数存起来。在 inference 时先按 int 来计算，最后结果再转换回 float32。这一过程中其实加速了大量的卷积计算。比如[这篇 paper](https://zhuanlan.zhihu.com/p/99424468) 就实现了 float32 到 int8 的量化。

[混合精度计算](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/0AltgP1ndt-ReJFB97UFKg)：上面讲的方法是用在 inference 阶段的，其实在模型训练时也可以用类似的方法来加速，只不过再用 int 就不大行了。一种比较新的方法是用 float16（也就是俗称的半精度），fp16 占用空间是单精度 (fp32) 的一半，双精度 (double，也就是 fp64) 的 1/4。

量化的具体实现方法可以参考[这里](https://zhuanlan.zhihu.com/p/58182172)。NVIDIA 专门推出了针对 inference 阶段量化加速的工具包 TensorRT

##### **2.2 新硬件 / DL Acclerator**

在纯硬件方面针对 DL workload 的工作也有很多，这里来看几个 parallel 相关的技术。最近 Data-Level Parallelism 不仅在深度学习中，在其他一些领域（比如[数据库](https://link.zhihu.com/?target=http%3A//www.vldb.org/pvldb/vol13/p226-fang.pdf)）也有了越来越多的应用。

- CPU：尽管 GPU 已经成了深度学习计算的标配，有时候仍然是需要 CPU 运算的。例如要在手机等辣鸡设备上进行 inference。

- - SIMD：SIMD 的含义是同一条指令在多个数据流上操作，和在向量处理器中一样。在具体实现中（例如 [SSE 指令集](https://link.zhihu.com/?target=https%3A//blog.csdn.net/gengshenghong/article/details/7008704)）是把一个 128 位 SSE 寄存器（这是新增加的 SIMD 专用寄存器，和早期[借用 FPU 寄存器的 MMX](https://zhuanlan.zhihu.com/p/31271788) 不同。在 SSE 指令集中是增加了 8 个这种寄存器）划分成 4 个块，同时存放 4 个 float32 单精度浮点数，4 个块可以同时进行运算（有 **多个运算单元**，作用于不同的地址），这样就提高了并行度。后来的 SSE2 / SSE3 / SSE4 / AVX 指令集在此基础上又增加对 float64 / 更多运算的支持，以及扩展了 SIMD 专用寄存器的位数，但本质上还是一样的。　　另外，SIMD 带来的并行和[超标量处理器](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pdev/p/11756069.html)的并行性（一个周期 issue 多个指令，用于 instruction level parallelism）[不是一个概念](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/SIMD)。非超标量处理器也可以 SIMD，而超标量处理器可以更并行 issue 多个 SIMD 操作。
  - VLIW：和一次 issue 多条指令，然后靠硬件进行 ILP 调度（也叫动态多发射。需要硬件实现乱序执行、分支预测等操作）的超标量处理器不同，VLIW（Very Large Instruction Width，采用这种技术的处理器也叫做静态多发射处理器）的含义是一次只 issue 一条**可以完成多个操作的复杂长指令**（也叫发射包，其实从软件的角度看是多条指令的集合）。因此一条指令的位宽可以很大。VLIW 是通过编译器来进行指令级并行调度的（比如一个常用的方法是[循环展开](https://zhuanlan.zhihu.com/p/37582101)，通过识别出可并行的重叠跨循环体指令块来实现 ILP）。VLIW 的本意是希望在编译阶段就识别出程序中的依赖关系（静态调度），得到可以并行执行的发射包，硬件只需要根据调度好的发射包直接执行即可，这样就简化了硬件实现，从而实现更大宽度发射包的并行执行。intel Itanium 的 IA64 指令集就使用了这个技术，但它[在当年并没有取得成功](https://zhuanlan.zhihu.com/p/101538383)。一个重要的原因是它只适合计算密集、算法固定可控的 workload。传统的通用应用程序可能很难具备这个属性（有很多 run-time 才能确定的值，另外 cache 访问也是不确定的），但深度学习任务具备这些性质。

- GPU：GPU 的本质可以看做 SIMT（Single Instruction Multiple Threads）。

- - GPU 集群：DL 框架一般都支持 GPU 和分布式训练，已经可以在 GPU 集群环境下运行了，但实际上还存在[一些问题](https://zhuanlan.zhihu.com/p/30976469)导致分布式场景下资源的使用率提不上去：1). CPU 和 GPU 之间 memcpy 开销太大、2). 参数通信开销太大、3). 显存不够用、4). GPU 很难虚拟化 (多任务共享)、5). 需要针对 ML workload 的更好的集群调度策略。 对于 1 和 3 其实也可以用前面提到的神经网络压缩、模型并行等方法解决； 对于 2 一个解决方案是尽量让[计算和通信在时间上重叠起来](https://www.zhihu.com/question/31999064/answer/54185461)，参考 ATC17 的 [Poseidon](https://zhuanlan.zhihu.com/p/30976469)； MSR 对于 5 做了很多工作，一方面是对大规模 GPU 集群上的真实日志数据进行分析，得出了一些经验（ [发表在 ATC19](https://link.zhihu.com/?target=https%3A//www.usenix.org/system/files/atc19-jeon.pdf)）。另一方面是设计一些更好的 scheduling 策略，例如 OSDI2018 的 [Gandiva](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/VvAykyrWS1mH3g52-6SacQ)（针对 DL workload 自身的特点来提高 GPU 集群使用率）和 NSDI2019 的 [Tiresias](https://www.zhihu.com/question/326552276/answer/706412167)； 对于 4 目前还没啥很好的解决方案，但可以通过一些 [软调度方案](https://zhuanlan.zhihu.com/p/57101049)来模拟。
  - 这学期 8205 课上会有 GPGPU 的 topic，到时候再补充

- 系统结构：这个和纯计算关系不是很大，可能暂时和 ML 加速也没啥关系（事实上目前在计算机网络研究中用的还多一些）...... 但对于优化整体性能会有帮助

- - NUMA：当单个 CPU 性能已经到瓶颈时，多处理器就成了比较好的解决方案。为了方便编程，需要保证能为应用程序提供跨越所有处理器的单一物理地址空间，这种也叫做共享内存处理器（Shared Memory Processor）。SMP 又可以分为两种类型：1) 任何处理器访问任何地址的仿存时间都是相同的，叫做统一存储访问（Uniform Memory Access）。 2) 对于每个核心，访问某些字会比访问其他字快一些，整个内存空间被分割并分配给不同处理器 / 内存控制器，这叫做非统一存储访问（NonUniform Memory Access，NUMA）。NUMA 虽然看起来复杂，但可以支持更大的规模（更多的核心），并且访问附近的存储器时具有较低的延迟。 在过去内存控制器还在北桥的时代，多处理器用的是 UMA（所有处理器都通过 FSB 总线连接北桥，再访问内存）。后来随着核心越来越多，为提高访存速度，内存处理器被做到了 CPU 内，每个 CPU 有（或者很少的几个核心共享）一个内存控制器，然后直连一部分内存空间，这些核心就被归为一个 NUMA node。而跨 NUMA node 之间的内存访问需要走 QPI 总线。可以参考[这里的图解](https://zhuanlan.zhihu.com/p/90624389)。 在一些涉及 many core 的工作中会经常用到 NUMA 的概念
  - RDMA：在网络环境中会用到。RDMA 全称是 Remote Direct Memory Access，用于实现不需要 OS 参与的远程内存访问（因为 message passing through kernel 会浪费本来很大的内存和网络带宽）。具体的技术细节可以[参考这里](https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_21125183/article/details/80563463)。不过最近（Eurosys2019）已经有了应用 RDMA 来加速分布式机器学习的[工作](https://link.zhihu.com/?target=https%3A//dl.acm.org/doi/10.1145/3302424.3303975)。

- 专用硬件：CPU 性能太菜，GPU 又太庞大，于是人们开发了 [AI 专用芯片](https://zhuanlan.zhihu.com/p/81141220)

- - FPGA：全称是 Field Programmable Gate Array，是可以多次烧写的。因为本质上属于软件所以可以快速开发 / 迭代。
  - ASIC：全称是 application-specific integrated circuits，出厂后电路就不可以改变了（需要流片）。但是性能比 FPGA 高。Google 的 [TPU](https://zhuanlan.zhihu.com/p/26882794) 就属于一种 ASIC。

##### **2.3 矩阵算子优化**

神经网络中的很多运算本质上就是对矩阵运算，因此可以用一些[矩阵乘法优化方案](https://zhuanlan.zhihu.com/p/101544149)来加速。比如 cublas 就是封装好的针对矩阵和向量运算的加速库，而对于神经网络加速则会使用 cudnn。

算子优化是个非常贴近 hardware 的工作，对多种设备都人工调优这些算子其实是比较难的... 如果能简化一部分工作就最好啦。于是就有了下面会提到的深度学习编译器。

##### **2.4 AutoML**

这个严格来说可能不算 MLsys 了... 但它的思路在很多 MLsys 问题中也会被用到。

AutoML 最早只能调很有限的几种参数，用的方法也比较暴力（启发式搜索）。后来能调的东西越来越多，方法也更加猛如虎... 一个里程碑是 [NAS](https://zhuanlan.zhihu.com/p/42924585)，标志着神经网络结构也可以 Auto 了。

常用的调参方法大致可以分为这几种：

1. 随机搜索，或者说叫启发式搜索。包括 GridSearch 和 RandomSearch。这种方法的改进空间主要体现在使用不同的采样方法生成配置，但本质上仍然是随机试验不同的配置，没有根据跑出来的结果来反馈指导采样过程，效率比较低。
2. Multi-armed Bandit。这种方法综合考虑了 “探索” 和 “利用” 两个问题，既可以配置更多资源（也就是采样机会）给搜索空间中效果更优的一部分，也会考虑尝试尽量多的可能性。Bandit 结合贝叶斯优化，就构成了传统的 AutoML 的核心。
3. 深度强化学习。强化学习在 AutoML 中最著名的应用就是 NAS，用于自动生成神经网络结构。另外它在 深度学习参数调优 中也有应用。它的优点是从 “从数据中学习” 转变为 “从动作中学习”（比如某个参数从小调到大），既可以从性能好的样本中学习，也可以从性能坏的样本中学习。但强化学习的坑也比较多，体现在训练可能比较困难，有时结果比较难复现。

之所以把 AutoML 也列出来，是因为这些方法在下面提到的 ML for system 问题中会很有用。比如之前做过的 [AutoTiKV](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247489834%26idx%3D1%26sn%3De78e59d44dcd3e4204f61b694e78c5b6%26chksm%3Deb163e40dc61b756e9c25786df399b75df377c9671e7ff6e61c429852d112b96ebb15886032e%26mpshare%3D1%26scene%3D1%26srcid%3D%26sharer_sharetime%3D1580199939978%26sharer_shareid%3Dc659855f06944f053b4dd00b09a6133b%26key%3D1ab9c3c2ddcd88d8e8df2d0e682769ce8390513da413c54ea3ff4b16886bc5a8837f51f58153d4f004785aa1cb56fadf825ab150036863ffc6bda2b0cb913fd396dc75044c8aa6827cd4eff23cc36bb4%26ascene%3D1%26uin%3DNzczMTcxOTQ0%26devicetype%3DWindows%2B7%26version%3D6208006f%26lang%3Den%26exportkey%3DAREfx6F5Og8IpQHwLYA%2BSDs%3D%26pass_ticket%3DnaBhvuZe5Y83cZAr3Yiei2RKqcK4eP9jz10Ty8uqijxDwO%2FFBaOxMAHgrmnZJsuW) 就应用了一种贝叶斯优化方法来调节数据库参数。

[cs294](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-sp19/assets/lectures/lec10/automl.pdf) 中给出了几个可提高的方向：

- Accelerate data collection and preparation

- - Automatic data discovery
  - Distributed data processing, esp. for image and video data
  - Data cleaning and schema driven auto-featurization

- Accelerate model selection and hyper-parameter search

- - Parallel and distributed execution
  - Data and feature caching across training runs

- Provenance

- - Track previous model development to inform future decisions
  - Connect errors in production with decisions in model development



#### **3. 深度学习框架 / 系统设计**

和 Distributed Training 的区别是这里更关注一些工程上的东西（框架设计、API 设计等等）。一个 Deep Learning Framework 大致需要[以下几个元素](https://www.zhihu.com/question/326890535/answer/717865868)：

- 支持各种算子 (op) 和 tensor (data)
- 计算图的定义方式（动态 v.s. 静态）
- Auto Diff
- Optimizer（例如 Adam）
- 各种加速和优化的库：cudnn, openblas,mkl 等

##### **3.1 Deep Learning Framework**

[这一节](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec05/05_deep_learning_frameworks.pdf)重点关注这几个方向：

- **[Differentiable Programming](https://www.zhihu.com/question/265173352)**：如果用过 Keras 或者 [PyTorch](https://link.zhihu.com/?target=https%3A//github.com/meton-robean/Machine-Learning-System-Notes/issues/2) 就会记得它可以简单得像搭积木一样摞一个 NN 出来，只需要定义一个一个的层（前向传播逻辑）和损失函数就行了。而 NN 的训练需要 Backward Propagation / Forward Propagation，也就是计算微分，运算时 framework 可以根据定义好的计算图 [自动求导算梯度](https://link.zhihu.com/?target=https%3A//borgwang.github.io/dl/2019/09/15/autograd.html)。只要可微分就可以保证这个积木能摞出来，然后使用链式法则就可以自动计算微分（ [Automatic Differentiation](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec05/05_deep_learning_frameworks.pdf)）。如果一个语言或者 framework 具备了 Differentiable Programming 的性质，就可以更简单的在它上面开发 Deep Learning 应用（可以类比 python 手写 NN 和 [Keras](https://link.zhihu.com/?target=https%3A//blog.csdn.net/wydbyxr/article/details/83956333) 的区别）。 [这篇文章](https://zhuanlan.zhihu.com/p/65488534)对 Auto Diff 的实现做了很详细的介绍。
- Embedded Domain Specific Languages：Embedded DSL 是在现有语言上（例如 Python）针对某个特定任务做的扩展。比如为了让 Python 做矩阵计算更方便发明了 numpy；为了进行机器学习就有了 TensorFlow / PyTorch 等等。Embedded DSL 的作用是完成 Linear Algebra -> Pipelines -> Differentiable Programs 的转化。
- 根据计算图的定义方式，可以分为 Declarative Abstraction（Embedded DSL 先生成静态计算图，类似编译执行 define-and-run，例如 Tensorflow、Caffe）和 Imperative（Embedded DSL 生成动态计算图并直接输出结果，类似解释执行 define-by-run，例如 PyTorch、Tensorflow Eager）

对于具体的 DL 框架来说，虽然很多公司都开始自研框架了，但最流行的基本就 TensorFlow、PyTorch、mxnet 等等那几家了。不过最近又出现了[分布式强化学习框架 Ray](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU2NDU4ODQwNA%3D%3D%26mid%3D2247483695%26idx%3D1%26sn%3D79eea870e3889cd963eccbe7167ad42f%26chksm%3Dfc49fb34cb3e72225ce38c611f4c3dd3191f89f00b272af02caa2580ec573df239b45fb343ab%26mpshare%3D1%26scene%3D1%26srcid%3D%26sharer_sharetime%3D1579610076036%26sharer_shareid%3Dc659855f06944f053b4dd00b09a6133b%26key%3D6dc1e3ec383dbb135bf7a4c3b0468cc673c6a8b5e424ba34eaeb1454be0f0f17b3f86660aaaa37e86ca3e197f97e4cf6207b79e59efea932db54d522bddb78cede6e96af33cac5384070baac9c7d8f8f%26ascene%3D1%26uin%3DNzczMTcxOTQ0%26devicetype%3DWindows%2B7%26version%3D6208006f%26lang%3Den%26exportkey%3DATub4ZAkPkZp41qLVV60doI%3D%26pass_ticket%3DRh2hySkWzgDzzLlcxt3ghMZSxpEfSvvoF08nPW6%2BBwTi%2BQMKVNa3%2Bolydrl9iPHm)，也具有很好的落地潜能。

##### **3.2 Inference / Model Serving**

之前关注了很多训练 ML 模型中会遇到的问题。但实际应用场景里，inference（直接使用训练好的模型 predict）的次数会比 training 多很多，因此 inference 的性能也很重要。

Inference 可以再分为以下两种：

- Offline: Pre-Materialize Predictions：所有可能的 query 都是已知的，就事先 predict 好存起来。一般没有这么玩的...
- Online: Compute Predictions on the fly：根据用户的输入实时 predict。这才是最常见的场景

一个典型的 ML inference pipeline 大致涉及到以下工序：

- input data
- -> Preprocessing (比如图片要 resize)
- -> model prediction (有时候会同时用很多 model，还要 ensemble 起来)
- -> 输出结果，有时候还要处理一下

这个 pipeline 的衡量指标包括 Latency、Throughput 等（和传统的 system 问题一样呀）。[cs294](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec07/07_prediction-serving.pdf) 里列出了几个最近的工作，可以参考 [这里的 paper 解读](https://www.zhihu.com/question/292760866/answer/506195469)。个人感觉这里可做的坑不多.... 大多是修修补补...

##### **3.3 深度学习编译器**

这里值得提一下 TVM。[这篇文章](https://zhuanlan.zhihu.com/p/50529704)对 TVM 进行了非常详细的介绍。

简单的说 TVM 是在把训练好的 ML model 部署在不同设备上时用的，重点关注的是 Inference 而不是 Training（也就是[推理引擎](https://zhuanlan.zhihu.com/p/87392811)）。在这一过程中，模型本身可能用了不同的 framework 来写（比如 tensorflow / PyTorch / MXNet，本质区别在于使用的算子类型可能不一样），而要部署到的设备也可能有不同的硬件架构（比如 x86 / ARM / GPU / FPGA）。inference 的过程也就是将 framework X 写出来的 model 放在硬件 Y 上运行的过程，这一过程和编译器是非常相似的（将语言 X 写的程序编译到硬件 Y 上运行），这也就是深度学习编译器的含义。

为了设计一个高效的深度学习编译器，TVM 借鉴了传统编译器 LLVM 的设计思想：抽象出编译器前端 [高级语言 C/java -> IR ]，编译器中端 [ 优化 IR，这种是不同编译器平台共享的 ]，编译器后端 [ IR -> 目标硬件上的 binary ] 等概念，引入 IR (Intermediate Representation。深度学习问题中可以将计算图作为 IR，称为 Graph IR)。这样不同硬件 /framework 都对标同一套 IR，就避免了需要对每种硬件和 framework 排列组合适配的问题。TVM 主要解决的是后端的问题 [在目标硬件上高效运行 IR]。而前端的问题 [生成和优化 IR] 就交给深度学习框架们完成（针对这一步，在 TVM stack 中提供了 [NNVM](https://link.zhihu.com/?target=https%3A//docs.tvm.ai/dev/nnvm_overview.html)， [作用是](https://link.zhihu.com/?target=https%3A//aws.amazon.com/blogs/machine-learning/introducing-nnvm-compiler-a-new-open-end-to-end-compiler-for-ai-frameworks/) represent workloads from different frameworks into standardized computation graphs）。

TVM 是和硬件深度集成的，也就是需要针对每种硬件平台实现相关的 AI 算子（类似 NVIDIA GPU 上的 cuDNN）。然而人工调优这些算子的实现是很费精力的（特别是要针对不同形状的业务模型），这里面也有一些 knob 需要调整。为了让这个过程也能 ML 化，于是后来有了 [AutoTVM](https://link.zhihu.com/?target=https%3A//docs.tvm.ai/tutorials/autotvm/tune_relay_cuda.html%23sphx-glr-tutorials-autotvm-tune-relay-cuda-py)。

[cs294 sp19](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-sp19/assets/lectures/lec12/dl-compilers.pdf) 还提出了几个可能的 future work：

- \- Compilers are great at Ahead of Time scheduling, what about Just-In-Time scheduling?
- \- Any way we can share GPU in predictable way and maximize utilization for DNN inference?
- \- Can we optimize for “fitness” of the kernel when it’s executed along with other kernels instead of its latency?



#### **4. 用 ML 优化传统的 system 问题**

这里面的花样就更多了... 在上学期 [Jon 的 ML system 课](https://link.zhihu.com/?target=http%3A//www-users.cselabs.umn.edu/classes/Spring-2019/csci8980/)上有过较详细的接触。大部分是用 ML 去优化一个传统 system 问题中，一些需要人工经验调整、或者说可以从历史情况 learn 到一些东西的模块。比如[数据库参数](https://link.zhihu.com/?target=http%3A//www-users.cselabs.umn.edu/classes/Spring-2019/csci8980/papers/tuning.pdf)、[操作系统页表](https://link.zhihu.com/?target=http%3A//www-users.cselabs.umn.edu/classes/Spring-2019/csci8980/papers/virt_addr_trans.pdf)、[数据库索引](https://link.zhihu.com/?target=http%3A//www-users.cselabs.umn.edu/classes/Spring-2019/csci8980/papers/learned_indices.pdf)等等。一个模块可以被 ML 化的前提是它**必须是 empirical 的**，参考它在页表（OS 的工作集原理）、数据库（DBA 是个很吃经验的活...）中的应用。如果人工都看不出来啥规律就别指望它能 ML 了...

一般认为用 ML 优化 system 的思想是起源于 [Jeff Dean 在 NIPS2017 的 workshop](https://link.zhihu.com/?target=http%3A//learningsys.org/nips17/assets/slides/dean-nips17.pdf)。这方面的工作很多发表在纯 system 的顶级会议以及下属的 AI for xxx workshop 上，另外一些 AI 会议的 workshop 也会收录一些这方面的工作，比如 nips 2018 的 MLsys workshop。从 2017 年开始已经有很多坑被做过了，但个人感觉还是有一些搞头的。感觉可以从下面两个角度再来搞：

- 同样的 scenario，使用更合适的 ML 算法。注意这里是更合适，而不是更高大上猛如虎。

- - 比如这篇 [ML+Database 的 paper](https://link.zhihu.com/?target=https%3A//www.pdl.cmu.edu/PDL-FTP/Database/sigmod18-ma.pdf)，使用了 LSTM 来预测未来的 workload pattern，还要用 GPU 训练，但生产环境上要求数据库服务器也安个显卡是不现实的。工程上的一个解决方案是搞个集中式的训练集群（类似 OtterTune），在 DBaaS 的情况下这种方法倒是行得通，但在对外发布的数据库产品中就不行了。
  - 这里感觉可以参考早期 AutoML 的一些工作，因为它们本质是很类似的（都是调参嘛...）。传统方法有启发式搜索 / 贝叶斯优化。最近也有很多人用强化学习去搞，但还是存在太吃资源的问题...
  - 这方面对 ML 知识的要求高一点。

- 寻找 system 界更多可以 ML 化的场景。这个更适合专业的 system researcher 来做，对 ML 倒是略有了解即可。

- - 有一类思路是把 ML 深度集成到系统设计中，比如 Andy 在 [2019 年的 15-721](https://link.zhihu.com/?target=https%3A//15721.courses.cs.cmu.edu/spring2019/slides/25-selfdriving.pdf) 课上提到过 [Self-Driving Database](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pdev/p/11511374.html) 的概念，和之前用 ML 优化数据库的工作不同的是，Self-Driving DB 更关注如何把 ML 和 DB 深度集成，而不是搞一个又一个外挂的模块了。
  - 一个类似的工作是在 OS 领域：[https://engineering.purdue.edu/WukLab/LearnedOS-OSR19.pdf](https://link.zhihu.com/?target=https%3A//engineering.purdue.edu/WukLab/LearnedOS-OSR19.pdf) 。
  - 另外还有个工作是在 Key-Value Storage Engine 的领域：[https://arxiv.org/pdf/1907.05443.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1907.05443.pdf)。它提出了 Design Continuum 的概念：存储系统中的很多数据结构本质上是很像的（arise from the very same set of fundamental design principles），例如 B+tree, LSM-tree, LSH-table 等，但它们却有不同的应用场景（比如 KV Store 中 [用 LSM 就比 B+ Tree 更合适](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pdev/p/11277784.html)），很难有一个十全十美的设计。这说明它们有相互替换的空间。这样我们可以将不同数据结构的选择也作为存储系统的一个 knob，根据具体 workload 和硬件的情况来自动选择一个合适的底层数据结构（find a close to optimal data structure design for a key-value store given a target workload and hardware environment）。
  - 一个更宏观一些的思路是做 system and algorithm co-design，让任意计算机系统都能和 ml 深度集成。虽然具体的 target system 不一样，但其中有很多模块都是类似的（例如 training、inference、system monitor 等等）。针对这一目标 MSR 提出了 [AutoSys](https://link.zhihu.com/?target=https%3A//www.microsoft.com/en-us/research/publication/the-case-for-learning-and-system-co-design/)，对这些通用模块进行了整合。



#### **5. 其他**

方向不是很契合就先不看了... 等用到了再填坑

- **ML pipeline / lifecycle**：[https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec03/03_ml-lifecycle.pdf)
- **Privacy**：[https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec10/10_adversarial_ml.pdf](https://link.zhihu.com/?target=https%3A//ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec10/10_adversarial_ml.pdf)
- **图神经网络训练系统**：[https://www.msra.cn/zh-cn/news/features/2019-review-machine-learning-system](https://link.zhihu.com/?target=https%3A//www.msra.cn/zh-cn/news/features/2019-review-machine-learning-system) [ATC19 NeuGraph]



#### **需要的技能树**

这是从一些公司 ML System Research Scientist 岗位的招聘要求中整理出来的，更侧重 system 一些。

##### **System**

- 工程基础：C/C++、OO programming。[阅读源码](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/neopenx/p/5187586.html)是个很好的学习方式
- OS
- 分布式系统
- 编译原理。特别是编译器优化技术、LLVM、memory optimization。Parser 之类不喜欢也可以不看
- Computer Architecture。另外还需要了解：1.GPU 架构，例如显存分配机制、CPU 与 GPU 交互。 2.CPU、存储系统相关的新技术。 3. 有条件可以了解下深度学习专用硬件。
- 常见的并行计算框架，例如 MPI/OpenMP/CUDA
- ML framework 的底层原理，扒源码
- 工业界的一些新东西：例如 k8s、KubeFlow、ElasticDL

##### **ML**

- 机器学习基础
- 常见的分布式机器学习算法、DL 模型压缩、模型加速方法（根据具体方向而定）
- 数理基础不要太菜… 不要被人吐槽像没学过高中数学…

























