**TLDR (or the take away)**

- 频率学派 - Frequentist - Maximum Likelihood Estimation (MLE，最大似然估计)
- 贝叶斯学派 - Bayesian - Maximum A Posteriori (MAP，最大后验估计)

现代机器学习的终极问题都会转化为解目标函数的优化问题，MLE 和 MAP 是推导优化函数的核心。

频率学派认为世界是准确的，有一个本体，本体的值不变，我们要找到这个本体或其范围；

**贝叶斯学派认为世界是不确定的，人们对世界有一个预判，而后通过观察对这个预判进行调整**，我们要找到对这个概率最优的描述。输入是先验 (prior) 和似然 (likelihood)，输出是后验 (posterior)。

同样是抛硬币的例子，对一枚均匀硬币抛 5 次得到 5 次正面，如果先验认为大概率下这个硬币是均匀的 (例如最大值取在 0.5 处的 Beta 分布)，那么 ![[公式]](https://www.zhihu.com/equation?tex=P%28head%29) ，即 ![[公式]](https://www.zhihu.com/equation?tex=P%28%5Ctheta%7CX%29) ，是一个 distribution，最大值会介于 0.5~1 之间，而不是像频率学派那样武断的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) = 1。

注意：

- 如果先验是 uniform distribution，则贝叶斯方法等价于频率方法。因为直观上来讲，先验是 uniform distribution 本质上表示对事物没有任何预判。
- 随着数据量的增加，参数分布会越来越向数据靠拢，先验的影响力会越来越小。



**MLE - 最大似然估计**

Maximum Likelihood Estimation, MLE 是频率学派常用的估计方法！

假设数据 ![[公式]](https://www.zhihu.com/equation?tex=x_1%2C+x_2%2C+...%2C+x_n+) 是 i.i.d. 的一组抽样，![[公式]](https://www.zhihu.com/equation?tex=X+%3D+%28x_1%2C+x_2%2C+...%2C+x_n%29)。那么 MLE 对 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的估计方法可以如下推导：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Chat%7B%5Ctheta%7D_%5Ctext%7BMLE%7D+%26%3D+%5Carg+%5Cmax+P%28X%3B+%5Ctheta%29+%5C%5C+%26%3D+%5Carg+%5Cmax+P%28x_1%3B+%5Ctheta%29+P%28x_2%3B+%5Ctheta%29+%5Ccdot%5Ccdot%5Ccdot%5Ccdot+P%28x_n%3B%5Ctheta%29+%5C%5C+%26+%3D+%5Carg+%5Cmax%5Clog+%5Cprod_%7Bi%3D1%7D%5E%7Bn%7D+P%28x_i%3B+%5Ctheta%29+%5C%5C+%26%3D+%5Carg+%5Cmax+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Clog+P%28x_i%3B+%5Ctheta%29+%5C%5C+%26%3D+%5Carg+%5Cmin+-+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Clog+P%28x_i%3B+%5Ctheta%29+%5Cend%7Balign%2A%7D)

最后这一行所优化的函数被称为 **Negative Log Likelihood (NLL)**。我们经常在不经意间使用 MLE，例如

- 上文中关于频率学派求硬币概率的例子，其方法其实本质是由优化 NLL 得出。本文末尾附录中给出了具体的原因 :-)
- 给定一些数据，求对应的高斯分布时，我们经常会算这些数据点的均值和方差然后带入到高斯分布的公式，其理论依据是优化 NLL
- 深度学习做分类任务时所用的 cross entropy loss，其本质也是 MLE



**MAP - 最大后验估计**

Maximum A Posteriori, MAP 是贝叶斯学派常用的估计方法！

同样的，假设数据 ![[公式]](https://www.zhihu.com/equation?tex=x_1%2C+x_2%2C+...%2C+x_n+) 是 i.i.d. 的一组抽样，![[公式]](https://www.zhihu.com/equation?tex=X+%3D+%28x_1%2C+x_2%2C+...%2C+x_n%29) 。那么 MAP 对 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的估计方法可以如下推导：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Chat%7B%5Ctheta%7D_%5Ctext%7BMAP%7D+%26%3D+%5Carg+%5Cmax+P%28%5Ctheta+%7C+X%29+%5C%5C+%26%3D+%5Carg+%5Cmin+-%5Clog+P%28%5Ctheta+%7C+X%29+%5C%5C+%26+%3D+%5Carg+%5Cmin+-%5Clog+P%28X%7C%5Ctheta%29+-+%5Clog+P%28%5Ctheta%29+%2B+%5Clog+P%28X%29+%5C%5C+%26%3D+%5Carg+%5Cmin+-%5Clog+P%28X%7C%5Ctheta+%29+-+%5Clog+P%28%5Ctheta%29+%5Cend%7Balign%2A%7D)

其中，第二行到第三行使用了贝叶斯定理，第三行到第四行![[公式]](https://www.zhihu.com/equation?tex=P%28X%29) 可以丢掉因为与 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 无关。**注意 ![[公式]](https://www.zhihu.com/equation?tex=-%5Clog+P%28X%7C%5Ctheta+%29) 其实就是 NLL，所以 MLE 和 MAP 在优化时的不同就是在于先验项 ![[公式]](https://www.zhihu.com/equation?tex=-+%5Clog+P%28%5Ctheta%29) 。**好的，那现在我们来研究一下这个先验项，假定先验是一个高斯分布，即

![[公式]](https://www.zhihu.com/equation?tex=P%28%5Ctheta%29+%3D+%5Ctext%7Bconstant%7D+%5Ctimes+e%5E%7B-%5Cfrac%7B%5Ctheta%5E2%7D%7B2%5Csigma%5E2%7D%7D)

那么， ![[公式]](https://www.zhihu.com/equation?tex=-%5Clog+P%28%5Ctheta%29+%3D+%5Ctext%7Bconstant%7D+%2B+%5Cfrac%7B%5Ctheta%5E2%7D%7B2%5Csigma%5E2%7D) 。至此，**一件神奇的事情发生了 -- 在 MAP 中使用一个高斯分布的先验等价于在 MLE 中采用 L2 的 regularizaton！**



## **附录**

**为什么说频率学派求硬币概率的算法本质是在优化 NLL？**

因为抛硬币可以表示为参数为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+) 的 Bernoulli 分布，即

![[公式]](https://www.zhihu.com/equation?tex=P%28x_i%3B+%5Ctheta%29+%3D%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Bll%7D+%5Ctheta+%26+x_i+%3D+1+%5C%5C+1+-+%5Ctheta+%26+x_i+%3D+0+%5C%5C+%5Cend%7Barray%7D+%5Cright.+%5C+%3D+%5Ctheta%5E%7Bx_i%7D+%281-+%5Ctheta%29%5E%7B1-x_i%7D)

其中 ![[公式]](https://www.zhihu.com/equation?tex=x_i) = 1 表示第 ![[公式]](https://www.zhihu.com/equation?tex=i) 次抛出正面。那么，

![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BNLL%7D+%3D+-%5Csum_%7Bi%3D1%7D%5En+%5Clog+P%28x_i%3B+%5Ctheta%29+%3D+-%5Csum_%7Bi%3D1%7D%5En+%5Clog+%5Ctheta%5E%7Bx_i%7D+%281-+%5Ctheta%29%5E%7B1-x_i%7D)

求导数并使其等于零，得到

![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BNLL%7D%27+%3D+-%5Csum_%7Bi%3D1%7D%5En%5CBig%28%5Cfrac%7Bx_i%7D%7B%5Ctheta%7D+%2B+%281-x_i%29%5Cfrac%7B-1%7D%7B1-%5Ctheta%7D%5CBig%29+%3D+0)

即 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D+%3D+%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En+x_i%7D%7Bn%7D) ，也就是出现正面的次数除以总共的抛掷次数。