### **引言**

智能手机的普及和移动互联网的发展极大地加速了数据的生成过程，令数据呈现出爆炸式的增长，并给大数据的实时管理带来了前所未见的难题和挑战。例如，微信的月活跃用户已超过了 10 亿，用户之间的交互则会带来更大规模的数据，包括语音、视频、图片以及相关的文本等。数据的规模和复杂性还在高速增长，如社交网络每天以亿级别的发文 [[65](http://www.jos.org.cn/html/2019/1/5646.htm#b65)]、轨道交通应用形成的大规模定位与轨迹信息以及网络通信中的数据传播等。为了处理实时增长的大规模复杂数据，流数据的管理和相关系统的研究一直是学术界和工业界的热点问题，包括早期的关系型数据为主的数据流管理系统、近期在工业界普遍使用的流式计算系统以及目前广泛关注的对图数据流管理系统的探索.

### **主要研究问题**

- 流数据采样。**基于有限的存储来管理无限的动态数据**是流数据管理中的基本挑战之一，应对这一挑战的最经典的思路则在于**流数据上的高效采样**。将高速更新的流数据采样到有明确规模边界的有限存储中，通过对采样数据的计算和挖掘来反映流数据所蕴含的重要信息。一方面，需要研究不同流数据场景下采样策略的选取，进而能够利用有限的资源**尽可能地反映原流数据的特征信息**；另一方面，需要结合计算需求，精准分析采样数据上的计算与挖掘结果相对于精确解的近似程度，**控制计算结果的偏移范围**；
- 持续性数据查询。流数据模型所对应的最核心的现实场景是实时监控。对不断生成的现实数据进行高效的计算挖掘，能够及时获取现实世界中的重要信息。例如银行对实时的交易数据进行监控，及时规避欺诈风险和追踪洗钱等违法行为。因此，给定基于结构特征、统计特征的数据查询模式，实时地监控流数据中匹配的目标，一直都是研究的热点。一方面，需要**保存已计算的中间结果来减少重复性的计算**，另一方面，又需要**避免中间结果维护带来过高的额外开销**；
-  流数据并行计算。应对流数据高速生成的一个重要策略就是利用数据和计算的独立性进行并行处理，提高系统吞吐量。系统日志数据、银行流水数据以及大量的移动应用产生的用户数据等在其初期的归整处理上都可以利用数据独立性进行流水线式的并行处理。在更复杂的数据计算和分析过程中，针对计算独立性和流场景的**一致性要求，设计锁机制来实现计算分析的并行化**。

### **国内外研究现状**

本节将通过流数据管理研究的 3 个不同阶段分别阐述流数据管理系统目前的研究脉络：首先，简单了解早期以关系型数据为主的数据流管理系统 (DSMS); 然后，详细介绍近期针对大规模复杂数据的流式计算系统；最后讨论目前兴起的对图数据流管理系统的探索。

#### **数据流管理系统**

数据流管理系统 (DSMS) 是指管理持续性数据流的计算机软件。不同于传统的数据库管理系统 (DBMS), 数据流管理系统支持**持续性查询**，每个查询从注册开始有效直至撤销结束，不仅仅执行一次。在有效期间内，随着数据流的不断更新，持续性查询的结果也会更新。传统的数据库管理系统一般假定有足够的外存用来持久化数据，并且可以进行随机访问。而数据流管理系统中主要强调用有限的内存来处理无限的数据流，并且只能顺序访问，这也是数据流管理最独特的特征以及最大的挑战。

目前，数据流管理系统并没有统一的系统框架，但在查询语言上，绝大多数都采用类似于传统数据库管理系统中 SQL 的声明式语言来表达查询，包括持续性查询语言 (continuous query language, 简称 CQL)[[66](http://www.jos.org.cn/html/2019/1/5646.htm#b66)]、流 SQL[[67](http://www.jos.org.cn/html/2019/1/5646.htm#b67)] 等。流查询语言也会支持窗口表达。在图示化的查询语言中，每个细分的查询由一个 “查询盒” 表达，各个细分查询的关联与组合通过 “查询盒”[[68](http://www.jos.org.cn/html/2019/1/5646.htm#b68)] 间的箭头连线表达。这个结构可以理解为流计算框架 (诸如 Storm 等) 中数据处理与传输架构的前身。

目前，主要的数据流管理系统有 STREAM[[69](http://www.jos.org.cn/html/2019/1/5646.htm#b69)]、Aurora[[68](http://www.jos.org.cn/html/2019/1/5646.htm#b68)]、TelegraphCQ[[70](http://www.jos.org.cn/html/2019/1/5646.htm#b70)]、NiagaraCQ[[71](http://www.jos.org.cn/html/2019/1/5646.htm#b71)] 以及 Gigascope[[72](http://www.jos.org.cn/html/2019/1/5646.htm#b72)] 等.STREAM 是斯坦福大学研发的基于关系模型的多功能数据流管理系统，它聚焦在数据流计算时的内存管理以及近似查询.Aurora 是一个以工作流为导向的数据流管理系统，用户可以通过 “查询盒” 来定义查询计划，每个 “查询盒” 含有基础的操作命令，“查询盒” 之间的数据流指向决定了各个步骤结果的传输框架.TelegraphCQ 是一个由伯克利大学开发的自适应数据流管理系统，用于支持不同场景的数据流应用.NiagaraCQ 是针对动态 Web 内容进行持续性 XML-QL 查询的数据流管理引擎 (XML-QL 是 XML 查询语言的一种扩展，用来支持大 XML 文档上的数据抽取，能够跨多个不同的 DTD 解释 XML 数据以及集成多个不同源的 XML 数据). 它对 XML 数据的抽取、查询与监控分别由 3 个主要组件来支撑：搜索引擎、查询引擎以及触发管理.Gigascope 是面向网络数据流监控的分布式数据流管理系统，可以用来支撑网络流量分析、入侵检测、路由配置分析等，也能够进行网络搜索、性能监控等.

表 4 给出了数据流管理系统的对比情况.这些系统的核心初衷在于对静态数据管理系统的流模型扩展, 因
此, 这些系统在查询语义和执行计算的数据处理逻辑方面与传统的数据管理模型有很大的重叠, 可以认为是在
传统数据管理系统的语义和架构上的扩展, 以支撑数据流场景的持续性查询.目前, 大规模高速生成的数据结构
复杂, 基于关系模型的数据流管理系统难以应对这种大数据场景.

![image-20210413221219832](2021-04-13-%E6%B5%81%E5%A4%84%E7%90%86%E7%B3%BB%E7%BB%9F.assets/image-20210413221219832.png)

#### **流计算框架**

流计算系统是目前学术界和工业界广泛使用的进行大规模数据处理的计算系统。目前，主流的流计算框架主要有 5 个：Storm[[73](http://www.jos.org.cn/html/2019/1/5646.htm#b73), [74](http://www.jos.org.cn/html/2019/1/5646.htm#b74)]、Spark Stream[[75](http://www.jos.org.cn/html/2019/1/5646.htm#b75)]、Samza[[76](http://www.jos.org.cn/html/2019/1/5646.htm#b76), [77](http://www.jos.org.cn/html/2019/1/5646.htm#b77)]、Flink[[78](http://www.jos.org.cn/html/2019/1/5646.htm#b78), [79](http://www.jos.org.cn/html/2019/1/5646.htm#b79)] 以及 Kafka Stream[[80](http://www.jos.org.cn/html/2019/1/5646.htm#b80)]. 流计算框架具有两个重要概念：交付保证 (delivery guarantee)[[81](http://www.jos.org.cn/html/2019/1/5646.htm#b81)]、流处理类型中的实时处理流和微批量处理流.

交付保证是系统在处理新来的数据项时提供相应层次的处理保障，分为 3 种：第 1 种是 “至少 1 次”, 也就是说，即便出现系统宕机等错误，也仍然能够保证新来的每个数据项被处理 1 次，可能会出现多次重复处理；第 2 种是 “最多 1 次”, 也就是说，新来的数据最多会被处理 1 次，在宕机等错误情况发生时，有可能数据会被丢弃而导致没有被处理；第 3 种则是 “恰好 1 次”, 也就是要求最严格的交付保证，确保无论发生什么情况，新来的数据项有且仅有 1 次处理.

流处理类型 [[82](http://www.jos.org.cn/html/2019/1/5646.htm#b82)] 的实时处理流是指每个新来的数据项都会被立即处理而无需等待后续的数据项，代表流框架有 Storm、Samza、Flink 以及 Kafka Stream; 微批量处理流是指数据项并不是到达之后立即处理，而是等待一段很短的时间使其聚成一定单位大小的单个小批量数据后再处理，对应的流框架有 Spark Stream 以及 Storm-Trident (Storm 的一个扩展，一个以实时计算为目标的基于 Storm 的高度抽象。它在提供处理大吞吐量数据能力 (**每秒百万次消息**) 的同时，也提供了低延时分布式查询和有状态流式处理的能力).

Storm 是一个 Twitter 开源流系统，也是最早出现的开源流式计算框架。在初始化时，需要用户定义一个实时计算框架，其结构是一个有向图。图中的点是集群中的计算节点，而边则对应整体计算逻辑中数据的传输，这个图框架也被称为拓扑。在一个拓扑中，传输的数据单元是一系列不可修改的键值对 (tuple), 键值对从 spout (消息源) 点中输出形成流数据并传输到 bolts (消息处理者) 点中进行计算，进而产生出新的输出流.bolt 输出流也可以传输给其他 bolts 节点，形成流水线式的计算处理流.Storm 也有不足之处，它并不支持状态管理以及窗口、聚集等操作，支持的交付保证为 “至少一次” 而不是高要求的 “恰好一次”.**Storm 的容错机制是** **Ack 机制**，通信过程中需要的额外开销可能会影响流计算的吞吐量.

Spark Stream (又称为 Structured Stream) 其实是 Spark[[83](http://www.jos.org.cn/html/2019/1/5646.htm#b83), [84](http://www.jos.org.cn/html/2019/1/5646.htm#b84)] 核心 API 的一个扩展，其对流式处理的支持其实是将流数据分割成离散的多个小批量的 RDD 数据 (RDD 是 Spark 的数据单元), 然后再进行处理。这些小批量的数据被称为 DStream (D 为 Discretized, 即**离散化**的意思).**Spark Stream 采用的是 Lambda[[85](http://www.jos.org.cn/html/2019/1/5646.htm#b85), [86](http://www.jos.org.cn/html/2019/1/5646.htm#b86)] 架构**，即同时运行批量处理和实时流处理的架构，其中，批量处理用来确保计算的正确性，而实时流处理则是为提高吞吐量。当实时流处理计算结果与批量处理的计算结果不一致时，则会校正错误，因为有批处理的存在，所以自然而然就实现了容错机制. Spark Stream 支持的是 “恰好一次” 的交付保证，而且与 Storm 一样，Spark Stream 并没有状态管理.

Samza 系统处理的流数据单元是类型相同或相近的消息，这些消息在产生之后是不可修改的。新产生的消息将被追加到流中，而流中的消息也不断地被读取。每条消息可以有相应的键值，这些键值可以被用来对流中的所有消息进行分割.Samza 的工作过程是按需计算转换 (或过滤) 一组输入流中的消息数据，并将计算结果以消息数据的形式附加到输出流中。因此，运行工作负载可能包含多个工作组，工作组之间可能有流数据的依赖关系，进而将整体计算抽象成一个数据流图架.Samza 的一大特色在于对状态管理的良好支持，可以用来支撑流数据的连接操作，其状态管理的引擎主要是 RocksDB[[87](http://www.jos.org.cn/html/2019/1/5646.htm#b87)] 和 **Kafka Log**.

Flink 与 Storm 相似，其整个数据处理过程被称为 Stream Dataflow, 既定的数据流动框架类似于 Storm 的拓扑.Flink 提取数据流的操作 Source Operator、数据转换 (map, aggregate) 的操作 Transformation Operator 以及数据流输出的操作 Sink Operator 与 Storm 架构中 Spout 与 Bolts 之间、Bolts 和 Bolts 之间的数据流是高度对应的，区别在于容错机制：Flink 的容错机制是 Checkpoint, 通过异步实现并不会打断数据流，因此 Checkpoint 的开启与关闭对吞吐量的影响很小；Storm 采用的是 Ack 机制，开销大而且对吞吐量的影响明显。另外，Flink 提供的 API 相对 Storm 更高级.Storm 的优势主要是具有比较成熟的社区支撑和经过较长期迭代之后的稳定性。目前，Flink 成熟度较低，仍有部分功能需加以完善，如在线的动态资源调整等.Flink 提供的交付保证是 “恰好一次”, 优于 Storm 的 “至少一次”.Flink 广泛受到大公司的亲睐，包括 Uber 和阿里巴巴，其中，阿里巴巴开发了基于 Flink 的流计算系统 Blink, 并应用在电商流数据计算中.

Kafka Stream 是 Apache Kafka[[80](http://www.jos.org.cn/html/2019/1/5646.htm#b80), [88](http://www.jos.org.cn/html/2019/1/5646.htm#b88)] 中的一个轻量级流式处理类库，用来支撑 Kafka 中存储数据的流式计算与分析。利用这个类库计算的结果既可以写回 Kafka, 也可以作为数据源向外输出。目前的流式计算系统基本都支持以 Kafka Stream 的输出作为数据源，如 Storm 的 Kafka Spout.Kafka Stream 作为一个 Java 类库而非系统，是流计算的重要工具之一。它可以非常方便地嵌入任意 Java 应用中，也可以任意方式打包和部署，除了 Kafka 之外，并没有任何外部依赖。与流计算系统相比，使用 Kafka 所受到的逻辑限制较少，开发者能够更好地控制和使用 Kafka Stream 上的应用.Kafka Stream 提供的是 “恰好一次” 的交付保证，并且能够利用 RocksDB 进行状态管理。目前，大公司在 Kafka Stream 上的实践较少，相关技术有待进一步成熟.

这些流系统最主要的相似性是针对数据独立性设计分布式并行计算策略，即：针对流式的输入，利用集群进行协作计算，而整体的数据依赖框架一般是一个有向无环图。集群的整体协作更像是流水线的协作，计算框架中的数据依赖所形成的数据流动方向基本上是单一既定的。除了数据处理的先后关系外，这些流系统对不同工作组之间的数据独立性往往要求更高，可实现较好的并行效果.

已有针对这些流系统的基准比较 (benchmarking)[[89](http://www.jos.org.cn/html/2019/1/5646.htm#b89)] , 然而系统的参数各不相同，同一系统在不同参数下的性能也会大相径庭，所以 benchmarking 的结果可信度不足 (见[表 5](http://www.jos.org.cn/html/2019/1/5646.htm#Table5)). 目前，从流系统支持的功能上来看，Flink 的功能是最完备的，有状态管理、高吞吐量和低延迟、支持最严格的 “恰好一次” 交付保证、可调控内存使用等，并且不会出现容错机制影响吞吐量的情况 (如 Storm). 虽然 Flink 的社区积累较少，相关 API 不够成熟，但在 Uber、阿里巴巴等公司的使用和推广之下，目前逐渐成为广受欢迎、功能强大的流计算框架.

![image-20210413222337872](2021-04-13-%E6%B5%81%E5%A4%84%E7%90%86%E7%B3%BB%E7%BB%9F.assets/image-20210413222337872.png)

### **图数据流管理系统的探索**

目前的流数据除了规模大和增长快之外，还有结构复杂的特点，而图模型能够以简易的形式表达出丰富的语义，因此，图模型与流数据模型融合而成的图数据流模型应运而生 [[90](http://www.jos.org.cn/html/2019/1/5646.htm#b90)]. 图数据流的多种定义可以用无限增长的边序列来概括 (孤立点的现实意义有限), 对应的研究问题除了传统的图计算问题之外，还有针对时间先后信息定义的研究问题，如满足时序约束的路径、子图查询等 [[91](http://www.jos.org.cn/html/2019/1/5646.htm#b91)]. 常见的时序约束有时间先后、时间间隔。例如，阿里巴巴通过网购交易数据中的环形子图的实时监控来追踪通过网购进行恶意信用卡套现的行为 [[92](http://www.jos.org.cn/html/2019/1/5646.htm#b92)] . 如[图 1](http://www.jos.org.cn/html/2019/1/5646.htm#Figure1) 所示：一个信用卡恶意的套现模式中，套现者向商户发起信用卡虚假购买，银行将真实的资金支付给商户之后，商户通过中间人将资金回流到套现者储蓄卡中，实现恶意套现。整个流程中，参与的对象和资金的流向构成了图的结构，而每个行为环节有其明确的时间先后关系。因此，针对图数据流的管理能够解决很多现实中的重要问题.

![image-20210413222941546](2021-04-13-%E6%B5%81%E5%A4%84%E7%90%86%E7%B3%BB%E7%BB%9F.assets/image-20210413222941546.png)

**在图数据流的管理方法上，核心的思路仍是在于利用已计算出来的结果来加速当前的计算，并且需要将中间结果维护上的时间和空间代价控制在可接受的范围内** [[93](http://www.jos.org.cn/html/2019/1/5646.htm#b93)]. 以流数据上的子图匹配为例，如果采用静态算法构建复杂的索引的思路来加速查询，则需要针对复杂的索引设计高速更新的算法。然而往往对查询的加速容易增加更新的代价，在无索引的极端情况下，针对子图的匹配需要完全重算；而在另一种极端情况下，即构建复杂的索引时，往往需要高额的更新时间甚至整个索引重构。因此，图数据流下的计算首要考虑的是**计算结果的维护与计算加速的权衡**。此外，在图数据流的高速更新场景下，**多线程的并发计算**仍然具有重要的意义。然而，**图数据中不同部分的关联程度较高，如单条边的删除能够导致大量路径特征的改变等**，因此，**在图数据流下进行并行算法设计和并发访问控制等具有严峻的挑战**.

基于目前已有的图数据管理的探索，可以总结出图数据流管理系统所需要解决的三大重要问题.

- 第 1 个是对图数据流中数据的基本操作的支撑，包括边序列的存储、增删改查以及已获取图数据的基本访问操作，如节点度数、邻居等；

- 其次是针对图数据流上的更复杂的挖掘和查询支撑，包括边流行为分析、路径计算以及更复杂的子图结构匹配等。对于复杂查询和挖掘的支撑，所设计的索引一方面需要考虑对计算的加速保证，另一方面需要考虑在高速更新场景下对索引的更新维护代价；

- 第 3 个问题则是**事务管理和并发、并行调度**等机制的设计，旨在提高系统的吞吐量和缩短响应时间.

### **总结与展望**

已有的数据流管理系统主要是在传统的数据流管理模型和架构上作了持续性查询的简单扩展，两者在语义和计算逻辑方面有相似之处，大部分数据流管理系统来自高校科研团队，而且这些数据流管理系统已经难以处理大规模复杂数据流的查询和计算。主流的流计算框架采用分布式的计算方式，利用数据独立性的特点进行并行计算，与传统的数据管理模型有很大区别。对数据的格式要求不高，能够处理大规模的多种复杂数据流，有大量的社区支持以及大规模企业的实践与推广，大部分是来自开源社区而鲜有是学术界的科研团队开发的。目前，对大规模生成的复杂数据的计算处理基本上依赖于流计算框架。由于对数据的独立性要求较高，流计算框架不适于处理数据之间有高度关联关系的模型，因此，目前针对图数据流管理系统的研究受到了学术界和工业界的高度关注.









