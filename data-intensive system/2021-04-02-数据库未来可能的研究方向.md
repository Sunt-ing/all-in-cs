很多人觉得数据库发展这么久，理论如此成熟，科研里面可以做的东西少了。其实不然，在当下数据越来越爆炸的时代，挑战与机会是并存的。本文是对 Guoliang Li 老师在 CCF 走进校园的报告总结后的一份水文，供个人娱乐。

先回顾一下数据库的架构。从单体数据库，到主备来做高可用，再到 Oracle RAC 的多写，再到完全分布式架构的数据库。RAC 有一个共享存储层，支持多写，但是却在存储容量上受限。Spanner 系的 Share-Nothing 架构，网络交互比较多，所以比较依赖事务模型的优化。

Guoliang 有讲一个 RAC 架构比较难设计的点：cache 一致性。cache 一致性是指，更新操作一般是 WAL + 内存来避免同步随机写，A 可能把更新数据存在内存里了，还没来得及写入磁盘，但是 B 这时候收到一个读相关数据的请求，要怎么样保证能读到最新的数据（最新的数据在 A 的内存里）？

## 分布式数据库的挑战

### 分片 (28:18)

分布式数据库，一个很重要的关键点就是数据怎么分的问题。行存还是列存？这个比较好选择。有更多问题等着研究，比如

- 如何保证你选的 partition key 在数据增长的时候依然保持均匀分布？
- 如何选择 Partition 的列？是不是应该在 Query 的比较多的列上做 Partition？
- Join 的比较频繁的 2 个 Region，是不是一般放到一个节点比较好？（跨 Region 查询）

这里有人在 [SIGMOD](https://link.zhihu.com/?target=https%3A//dl.acm.org/doi/10.1145/3318464.3389704) 上用学习的办法去学 partition 应该怎么做。同样还有 Graph Key Partition。这里没听的特别明白，不过我觉得确实也大有可为，比如你本身就是 Graph Database，那么用 Graph Key Partition 也就很符合数据特性。或者说你用图的办法建模，例如每个节点表示一列，这样可以把频繁 Join 或者做事务的 2 个 Region 放到一个节点上。

以及还有一个研究关键点，partiton 完了之后，怎么 Evaluate 我的 partition 方法好不好？因为很有可能分片的方案非常多，不可能做到每一种新的 Partition 都完整跑 SQL Query 来评测。因此有没有一种 Evaluate Model，我给出一个分片方案，就能以比较数学的方式来评估方案的表现？我觉得如果有了这么一个优秀模型，对于 [Automatically Partition](https://link.zhihu.com/?target=https%3A//hstore.cs.brown.edu/papers/hstore-partitioning.pdf) 领域应该是一个非常好的助推剂。

### 事务 (34:20)

分布式的事务支持，是 Newsql 和传统分库分表方案的最重要区分点。有很多业务至今没有使用 Newsql，就是因为它们的事务模型比较简单，分库分表就能解决。而传统分布式数据库由于 2PC 本身的瑕疵，会带来一定的性能抖动。因此，这个模块怎么设计也是一大难题。

业界普遍使用的比较多的模型是 2PC。2PC 有很多种实现方式，比如各种优化。事务模型涉及到的概念比较多，比如怎么做并发控制，怎么控制写写冲突（一般通过锁），读写冲突（MVCC）。同时，数据库要支持哪些隔离级别也非常影响相关的设计。比较传统的方案，应该是通过统一发号器，每次查询前 / 事务开始前拿到一致性视图，比如 MySQL 就是这么做的。但是 TiDB 因为想要避免单点，没有实现类似的方案。PG 在 08 年的[论文](https://link.zhihu.com/?target=https%3A//courses.cs.washington.edu/courses/cse444/08au/544M/READING-LIST/fekete-sigmod2008.pdf)，提出了新的隔离级别 SSI，并且应用在系统中，使用的是图检测的方式。

分布式事务纷繁复杂，但是这里我主要讲发号器，它是整个事务的灵魂。一个准确，HA 的发号器对于事务是非常有用的，无论是判断数据可见性，一致性，化解数据冲突等等。而分布式架构带来新的难点，因为每台机器时间不一样，会有 time drift 的现象。因此这鼓励学术界提出更好的发号器。

目前比较好的应用有 [TSO](https://link.zhihu.com/?target=https%3A//www.cs.princeton.edu/courses/archive/fall10/cos597B/papers/percolator-osdi10.pdf)，逻辑时钟，[HLC](https://link.zhihu.com/?target=https%3A//cse.buffalo.edu/~demirbas/publications/hlc.pdf)，True Time etc.。HLC 既能通过逻辑时钟部分的设计来减少对中心化授时的需求，也能对所有的时间戳进行排序，目前几乎所有的时钟设计都是逻辑 + 物理。True Time 是从硬件层面做到误差控制。

总的来说事务这个领域理论的创新还是非常难，上面说的都是已有的方案，并没有什么新鲜东西。原子钟上云之后，是否能为业界提供更好的架构思路？值得探讨。

### 优化 (39:49)

我一直觉得优化器是整个数据库工程难度最大的部分之一。在分布式架构下，这种查询优化变得更加难了，分布式查询器有了无限的改进空间。

根据 Guoliang 老师的说法，在分布式的环境下了，传统单机的分析方法很可能不适用了：不同节点数据交互要考虑网络因素，A,B 上２个表要 join，是 A 传给 B 好呢，还是 B 传给 A 好？越复杂的分析，网络交互就越多，也越难以分析。

### 高可用 (41:18)

这一部分主要是业界研究的比较多，学术界因为没有相关场景所以比较难做。 数据库做高可用主要有这么几种办法： 

- 硬件冗余（多台机器，比如主备） 
- 软件冗余（多副本，两地三中心，异地多活，数据复制） 
- 自动诊断，预测（用机器学习的办法来监控数据库应用的情况，及时做调整）

感觉这个领域也没有什么新东西。比较新的概念就是 SDDB (Self-Driving Database)，大概就是收集数据库当前运行的性能指标，然后做一些自动化的参数调整之类的东西，比如阿里在 VLDB 发的这篇，就是根据当前 workload 调整 buffer size:

[http://www.vldb.org/pvldb/vol12/p1221-tan.pdfwww.vldb.org](https://link.zhihu.com/?target=http%3A//www.vldb.org/pvldb/vol12/p1221-tan.pdf)

以前这玩意是 DBA 手动调的。

## 硬件对数据库系统设计的影响

硬件一直在影响着数据库的设计。

- 想想我们为什么会有 WAL？是为了把随机写开销化成顺序写。
- 为什么会有 buffer pool？为了写操作和读操作更加迅速。
- 为什么我们要用 Page 作为 B+ Tree 的节点单位？为了简化磁盘读取方式。
- 为什么 Wisckey 火了？因为 SSD 的大规模使用，使得 LSM 的架构可以有更好的设计。

新的硬件出现，势必会影响数据库的设计，而这带来了新的研究热点。

### CPU(44:03)

目前由于硬件的限制，机器单核能力提升不大，不过数量越来越多。以后的时代，我们要面对的计算环境，很有可能是几千个 CPU。多核 CPU 反而带来性能下降，因为不同的核会争抢相同数据资源。比如 NUMA 架构下如何管理多核对同一块内存区域的访问是一个重要问题，因此如何控制对同一块数据资源的并发问题，或者说设计一个在重核情况下的新的[并发控制的算法](https://link.zhihu.com/?target=http%3A//vldb.org/pvldb/vol10/p49-wang.pdf)，就显得尤为重要。

### NVM(49:08)

> 插播相关硬件科普：闪存分很多种，其中一种叫做 NAND 闪存。NAND 闪存在 U 盘，存储卡，固态硬盘上都可以看到。U 盘和 SSD = 控制器 + 闪存，而影响闪存速度的主要是是控制器的性能。比如同为闪存存储，SSD 性能高于 U 盘，主要靠的不是材料方面的优越性而是控制器的实现。

3D Xpoint 就是一种 NVM 技术，Intel 基于这个技术提出了 Optane，美光科技提出了 QuantX。而且是 ByteAddressable 的。ByteAddressable 这个概念 + NVM 和普通 NAND 的读写差异，给了数据库研究人员很多想象空间。比如之前我往磁盘上读写的时候用的是 Page，现在有了和内存一样的 ByteAddressable 功能，那我可不可以换成 Record？降低数据粒度。

而且因为 NVM 的出现，我们也可以思考存储器的架构，比如 Memory -> NVM -> Disk 这样新的存储架构，就会带来新的思考点： 

- Log 要不要放 NVM？RTO 会不会快很多？  
- Index 要不要放 NVM？索引会不会快很多？
- 传统的 Write Ahead Log 是不是要换成 [Write Back Log](https://link.zhihu.com/?target=http%3A//www.vldb.org/pvldb/vol10/p337-arulraj.pdf)? 

### Network(55:49)

RDMA，直接访问对方内存 (Bypass CPU)。这项技术是分布式领域非常关键的技术，它又会带来哪些新研究方向呢？

- 事务。2PC 避免不了网络交互，RDMA 能极快提升事务处理能力，也给了新的事务处理模型想象空间。
- Offloading。把某些计算任务扔给其他硬件完成，解放 CPU，不被频繁中断。
- Programmable。网卡上直接通过编程的方法做条件过滤（不需要 CPU 参与，提高处理速度）
- Protocol。替换传统的，比较笨重的 TCP/IP 协议。

## 云原生数据库新趋势

云原生数据库里面曾经最火的明星产品可以说是 Amazon 的 Aurora。它的细节这里不讲，有兴趣可以看看另一篇[文章](https://zhuanlan.zhihu.com/p/186286403)。它的主要设计理念是降低 I/O，异步写 Page，并且把这个 task offload 到内部存储系统。存储计算解耦，可以做到分层扩容。比如说计算节点增加 500 个，存储节点只增加 100 个，按需分配。

云原生数据库有几个研究点: 

-  Near Data Processing。既然存储计算完全分离，这会加重一些新的问题，比如查询得到的大量数据，如果不在存储层进行处理，而是全部传输给计算节点，这会极大地浪费网络 I/O。因此，现在计算分离的架构里面，通常都会有模块做算子的下推。TiKV 里负责下推的是 Coprocessor 模块。这方面的难点是，底层的 KV 存储没有完整的关系表信息，因此在做一些优化的时候无能为力。
- 支持多写。在云原生数据库里支持多写是比较难的一件事情，比如 Aurora 就不支持（更新：是一开始的架构不支持，现在支持）。新的 PolarDB 是支持的。 
- 内存虚拟化。把内存做高可用，通过 RDMA 连接。这个可以联想到 PolarDB 1.0。背后的大理念是资源池化。以前单体的时候，所有数据库 kernel 都在一台机器上，现在抽离每个部分并且做成一个资源池，方便上云。资源池的优势是封装了 low level 的细节，通过相关资源调度算法达到高效率的利用率。
   

这些也不是比较新鲜的研究问题了，业界已经开始大规模做了。个人思考，数据库 + k8s 是不是一个研究方向呢？虽然说 k8s 的设计初衷是运行无状态运用，但是如果能管理状态的话，那么可以说就是使得云上应用更加规范化了。这个方向还是比较有争议的，大多数难点还是工程上的，从学术的角度发文章可能比较难。

## 总结

这份报告还是比较有研究价值的。我只不过截取了一些我自己听得懂的点，多听几遍就会觉得讲的还是比较泛，需要自己去深挖各个方向。









作者：Bowen Xiao
链接：https://zhuanlan.zhihu.com/p/268772524
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。