### NewSQL大致介绍

NewSQL对于OLTP能够像NoSQL那样横向拓展，同时提供传统RDBMS那样的事务ACID保证，使得应用程序逻辑与数据操作逻辑再次分离。和Vertica、Greeplum等数据仓库不同，NewSQL的读写基于索引、耗时短。

### NewSQL的分类

- 使用全新的架构，如：Spanner
- 基于MySQL、PostgreSQL的分片中间件
- 云服务商的DBaaS

#### 使用全新的架构

这类数据库系统都是基于 shared-nothing 的分布式架构，同时包含以下模块：

- 的multi-node concurrency control)
- 多副本数据复制 (replication)
- 分布式查询处理 (distributed query processing)

除了 Google Spanner 之外，它们通常自己管理存储模块，意味着数据分布在不同节点上，从而 “send the query to the data”，而如果依赖三方存储则意味着 “bring the data to the query”。

为了避免生态问题，它们通常会兼容现有的DBMS通信协议，如 Clustrix、TiDB兼容 MySQL，CockroachDB 兼容 PostgreSQL。

例子：Clustrix、CockroachDB、Google Spanner、H-Store、HyPer、SAP HANA、VoltDB、TiDB

#### 中间件实现透明sharding

ShardingSphere等中心化的中间件主要负责路由请求、协调事务执行、分布数据、复制数据以及划分数据到多节点。通常在每个数据库节点中还有一层薄薄的通信模块，负责与中间件通信、代替中间件执行请求并返回数据。

特点：

- 运行相同的单机版数据库系统
- 只包含整体数据的一部分
- 不用于单独接收读写请求

优点：

- 可以无感横向扩容

#### Database-as-a-Service

用户直接调用云服务商接管的数据库系统，根据资源利用情况付费

如：Amazon Aurora，它既兼容 MySQL和 PostgreSQL 的通信协议，背后使用基于 log-structured 的存储管理模块来提高 I/O 的并行效率。

### NewSQL 的设计思路

#### 利用内存

传统 DBMS 基于磁盘和SSD 或 HDD等块存储设备，仅少量使用内存。现在，很多数据库可以被完全装入内存中，不再需要缓存池管理、重量级并发控制机制等模块，但是内存数据库的研究早就展开了。

#### Partitioning/Sharding

所有 NewSQL 都通过 partitions 或 shards 来实现横向扩容。但分布式数据库早就有了，只是当时硬件价格高昂，没有高性能、高可用应用。那时候数据库的 QPS 普遍在几十到几百之间。

NewSQL 数据库的一个重要特性就是支持数据的线上迁移 (live migration)，它允许数据库在不同的物理节点上重平衡 (rebalance) 数据，缓解热点，以及扩容的同时，不影响数据库本身的服务。数据线上迁移的方案：

- 将数据组织成粗粒度的逻辑分片，散列到物理节点上，重平衡时在节点间移动逻辑分片。应用：Clustrix、AgilData、Cassandra 和 DynamoDB。
- 在更细的粒度上，如记录 (tuple) 或一组记录 (groups of tuple) 上，通过取值范围来重排数据。应用：MongoDB、ScaleBase、H-Store。

#### 并发控制

并发控制提供了事务的原子性和隔离性保证，分为：

- 中心化并发控制：所有事务操作的起点都是中心化的协调器 (coordinator)，由后者来决定是否同意操作
- 去中心化并发控制：每个节点都维持着访问自身数据的事务状态信息，通过节点间通信、协调来确定是否有并发冲突。去中心化协调器扩展性好，但要求节点间时钟高度同步以确定事务的全序。

##### 2PL（two phase locking）

主要是被20世纪70年代的首批分布式数据库系统使用。SDD-1 是第一个在 share-nothing 集群下支持分布式事务的数据库，它使用了中心化的协调器。IBM 的 R* 类似 SDD-1，不过它采用的是去中心化的协调机制，分布式 2PL，每个事务都会将其访问的数据加锁。INGRES 数据库的分布式版本也是使用分布式 2PL，但它依赖于中心化的死锁检测机制。

##### Timestamp ordering (TO)

由于解决死锁问题过于复杂，几乎所有 NewSQL 数据库都抛弃了 2PL。当前的流行的是 TO并发控制机制的不同变体，在该机制中数据库假设：不会按可能造成违背 serializable ordering 的操作顺序来执行并发事务。

在 NewSQL 系统中最流行的并发控制协议是去中心化的 MVCC，包括 MemSQL、HyPer、HANA 及 CockroachDB，尽管MVCC早就有了。

##### 2PL 和 MVCC 的融合方案

InnoDB、Google Spanner、NuoDB、Clustrix 以及所有中间件和 DBaaS 方案都继承了 2PL 和 MVCC 混合的机制。此方案中，写事务按照 2PL 机制去获取数据的锁，每当事务修改一条记录数据库就为该记录创建新版本；读事务则无需获取锁，因此也不会阻塞写事务。

Google Spanner (包含其后代 F1 和 SpannerSQL) 通过硬件设备 GPS和原子钟获得了高精度的时钟同步，从而为事务产生时间戳，获得事务的顺序，最终在广域网上实现多版本数据库的数据一致性。

CockroachDB 基于低同步的时钟和逻辑计数器的混合时钟协议也支持相同的事务一致性。

这些并发控制机制没有显著的创新，主要是老方法在现代硬件和分布式环境中的工程化。

#### 二级索引

支持二级索引对于分布式数据库来说并非易事。例如：假设有一张客户表，按照客户 id 分片到不同的物理节点上。当有查询想要根据客户的邮箱地址来查时，就需要去每个节点上都查一遍，才能得到正确结果。

一个分布式数据库要支持二级索引，需要考虑两个设计决定：

- 将二级索引存放在哪里
- 如何在事务中维护二级索引

如果一个系统中存在中心化的协调器，如中间件方案，那么可以将二级索引存放在协调器节点和分片节点上，这样全局只有一个版本的索引数据需要维护。

使用新架构的 NewSQL 数据库通常使用分片二级索引方案，即每个节点都存储索引的一部分，这样更新索引时只需要更新一部分，只是查询索引时要分派到各个节点上。

Clustrix 将二级索引按范围分片，每个节点都存着范围与分片的对应关系。遇到查询、更新请求时，都先将请求路由给合适的节点，再由后者执行相应的操作。这种两层的设计结合了两个方案的优点。

如果 NewSQL 数据库不支持二级索引，开发者的常见做法就是自己构建二级索引，并放在分布式缓存系统中，但依赖外部系统将使得索引与数据之间的行为没有一致性保证，需要谨慎对待。

#### Replication

在数据库复制上，有两个重要的设计决定：

**如何保证跨节点的数据一致性？**强一致系统要求新写入的数据被所有复制节点持久化之后，事务才能被认为已经提交。但 DBMS 维持这样的同步状态需要使用像 2PC 这样的原子提交协议来保证数据同步，如果在这个过程中有一个节点故障或者出现网络分区，数据库服务就会没有响应。这也是为什么 NoSQL 系统通常使用弱一致性 (weekly consistent) 或最终一致性 (eventual consistent) 模型：

- 在弱一致性保证下，通常 master 节点无需等待所有复制节点持久化数据就可以告诉认为事务以及提交。
- NewSQL 一般支持强一致性数据复制，但这些系统实现强一致性的方式早在 20 世纪 70 年代就已被研究了。

**如何执行跨节点数据传播？**主要有两种执行模式：

- active-active 复制：让每个复制节点都执行相同的请求。例如，接收到一个新请求，数据库系统会在所有复制节点上执行相同的请求。如果请求先后到达，容易导致数据不一致。
- active-passive 复制：请求先在一个节点上执行，再将状态传递给其它复制节点。

大多数 NewSQL 数据库系统采用 active-passive 复制，这主要是因为每个请求到达不同节点的顺序不同，如果直接使用 active-active 复制，很出现。

相较之下，deterministic DBMSs，如 H-Store、VoltDB、ClearDB，都使用 active-active 复制，因为在 deterministic DBMS 中，事务在不同节点上的执行顺序能够保证一致。

Spanner 和 CockroachDB 是论文发表截止前提供广域网数据同步一致性优化唯二的两个数据库。Spanner 采用了原子钟与 GPS 硬件时钟的组合方案，CockroachDB 则采用混合时钟的方案。

#### 故障恢复

主要有两种实现方案：

- 利用本地快照和 WAL 先恢复数据，然后从其它节点拉取新的日志。对于使用物理日志 (physical/physiological logging) 的数据库来说跟上整个集群是可能的，因为直接将数据的更新同步到本地比执行 SQL 速度要快很多。
- 放弃本地快照，直接从集群中的某个节点全量拉取数据。

### 未来展望

未来的数据库系统应该要能够在新产生的数据上执行分析型查询和机器学习算法，这种 workload 通常被称为 real-time analytics 或 HTAP，能够同时从历史数据和新数据中攫取洞见和知识，新数据价值高。

目前有 3 种支持 HTAP workload 的方法：

- 一套用于处理 OLTP workload 的前端数据库，另一套用于处理 OLAP workload 的后端数据库。前端数据库处理新事务产生的数据，后台用 ETL 工具将数据从 OLTP 数据库导入数据仓库。

- lambda 架构。用一套批处理系统，如 Hadoop、Spark，在历史数据上计算复合的视图，同时使用一套流式处理系统，如 Storm、Spark Streaming 等在新产生的数据上计算准实时的视图。

- 用一个 HTAP 数据库同时支持高吞吐、低时延的 OLTP 和在历史数据和新数据上运行逻辑复杂、时间长的 OLAP workload。它将一些特殊的 OLTP 实现方案 (in-memory storage、lock-free execution) 和 OLAP (columnar storage、vectorized execution) 方案结合。