#### 背景

一致性分类

- 强一致性：Paxos、Raft（muti-Paxos）、ZAB（muti-Paxos）
- 弱一致性：DNS系统、Gossip协议

实现：

- Google Chubby：分布式锁服务，采用了Paxos算法
- etcd：分布式键值数据库，采用了Raft
- ZooKeeper：分布式引用协调服务，采用了ZAB

Paxos 有啥用 ?

- 没有 Paxos 的一堆机器，叫做分布式；
- 有 Paxos 协同的一堆机器，叫分布式系统.

Paxos和Raft

- Google Chubby 的作者 Mike Burrows 说过：世上只有一种一致性算法，那就是 Paxos …

- Raft 的贡献在于把一致性算法落地。因为 [Leslie Lamport](https://link.zhihu.com/?target=http%3A//www.lamport.org/) 的理论很抽象，要想把他的理论应用到现实中，还需要工程师完全掌握他的理论再添加工程必要的环节才能跑起来.

Raft 的核心可以认为是 multi Paxos 的一个应用

网络上 Raft 比 Paxos 流行，因为 Raft 的描述更直白一些，实际上 Raft 比 Paxos 更复杂. Raft 详细的解释了”HOW”, 缺少”WHY” 的解释. Paxos 从根本上解释清楚了”WHY”, 但一直缺少一份通俗易懂的教程。以至于没有被更广泛的接受

分布式系统的一致性问题最终都归结为分布式存储的一致性。像 aws 的对象存储可靠性要求是 9~13 个 9. 而这么高的可靠性都是建立在可靠性没那么高的硬件上的.

- 磁盘4%年损坏率、服务器宕机时间：0.1%或更长、IDC间丢包率：5%到30%

#### 复制的策略

- 主从异步复制
  - 容易实现，但是存在一个问题：客户端收到一个**数据已经安全** (OK) 的信息，跟**数据真正安全** (数据复制到全部的机器上) 在时间上有一个空隙。因此它不是一个可靠的复制策略
  - 如：MySQL的binglog复制。主收到数据并应答OK，然后复制到从机。
- 主从同步复制
  - 可靠：直到所有数据都落地，才告知收到。但是，如果系统中有任何一个机器宕机，写入就进行不下去了。系统的可用性随集群的增大而降低。
- 半同步复制
  - master应答前要把数据复制到一定数量的机器上。
  - 好处：副本多时可靠性很高，1台机器宕机也不会让整个系统停止写入。
  - 问题：如果master将数据A复制到slave-1，然后将数据B复制到slave-2，然后宕机，则任何一个slave都不能提供完整的数据。所以，在整个系统中，存在每一种不一致性。
- 多数派读写
  - 为了解决半同步复制中数据不一致的问题，要求每条数据必须写入半数以上的机器里。每次读数据都必须检查是否半数以上的机器都拥有这条数据。
  - 问题：假设node-1、node-2都写入了a=x，然后node-2、node-3都写入了a=y，这时，如果一个读取a的客户端联系到了node-1、node-2，它将看到两条不同的数据
- 多数派读写，后写优胜
  - 给每笔写入加上一个全局递增的时间戳，如果大时间戳被看到了，就应该忽略小时间戳
  - 问题：如果客户端没有完成一次完整的多数派写入就挂掉了，如：假设node-1、node-2都写入了a=x，然后node-2写入了a=y，这时写入的客户端挂了。这时，如果一个读取的客户端读的是node-1、node-2，那它看到的是a=x；如果联系到的是node-2、node-3，那它看到的是a=y

#### 从多数派读写到 Paxos 的推导

- 一个假想的存储服务
  - 一个有3个节点的存储集群，采用多数派读写策略
  - 只存储变量 i，i每次更新都对应有多个版本
  - 这个存储系统对应多个命令
    - get：读最新的i
    - set(n)：将下一个版本的i设置成n
    - inc(n)：对i加n生成新版本
- 问题：如果两个客户端进程并发执行inc，则可能出现写覆盖问题
  - 解决办法：增加一个写前读取的操作：每次写某一个版本的数据前，必须确定是否有其他客户端在写，如果有，则放弃
- 问题：可能有并发问题，X、Y可能同时做这个写前读取的操作，然后发现没有其他进程在写
  - 解决办法：存储节点记住谁最后做过写前读取操作，同时只允许这个客户端后续写入
  - Paxos也是通过两次多数派读写实现强一致性

#### classic Paxos

- classic Paxos算法的特点：

  - 基于多数派读写
  - 每个Paxos实例用来存储一个值
  - 用两轮RPC来确认一个值
  - 一个值被确定（被多数派接受写入）后不能修改
  - 强一致性

  - 解决了网络延迟/乱序的问题，没有试图解决存储不可靠、消息错误的问题
    - 因为这两个问题跟分布式关系不大，属于数据校验层面
    - 可以使用Byzantine Paxos解决

- 概念

  - Proposer：发起Paxos的客户端进程
  - Acceptor：存储节点
  - Quorum：多数派，即半数以上的Acceptor
  - Round：每个Round包含两次多数派读写，分别是phrase-1、phrase-2；
  - rnd：每一轮的编号，单调递增
  - last_rnd：Acceptor记住的最后一个写前读取的Proposer，以此决定谁能写入
  - v：最后被写入的值
  - v_rnd：在哪个Round v被写入了

- phrase-1

  - Proposer向Acceptor发出带rnd写前读取请求
  - Acceptor如果请求中的rnd>last_rnd就将last_rnd更新为rnd，返回Accepted；否则拒绝，返回last_rnd和v
  - 收发的报文示例：
    - request：
      - rnd：int
    - response：
      - last_rnd：int
      - v：“XXX”
      - vrnd：int
  - Proposer等待收到多数（quorum）个应答，否则整个系统就hang住了
    - 因此Paxos只能处理少于半数的节点失效的情况

  - 当Proposer收到Acceptor发回的应答：
    - 如果所有应答中的v值都是null，即没有v将被写入:
      - Proposer可以在下一个phrase将其想写入的值写入
    - 否则：
      - 如果应答的last_rnd大于发出的rnd：退出
      - 否则从所有应答的vrnd中选择最大的，将其v做完自己的phrase-2的写入
        - 这样可以修复其他Proposer未完成的写入

- phrase-2

  - Proposer X将它要写入的值（它从应答中收到的值或它自己想写入的值）写入Acceptor
    - 这个写入可能会失败：因为在Proposer收到应答和发出phrase-2请求这之间，可能有其他Proposer完成了一个rnd更大的phrase-1
  - 格式
    - request
      - v：“xxx”
      - rnd：int
    - response：
      - OK：bool
  - Acceptor收到含有和本地的rnd相同rnd的phrase-2请求时就接受写入，否则拒绝

- Learner

  - Paxos中还有一个叫learner的角色，在phrase-3中被Acceptor告知一个值被确定了
  - 多数情况下一个Proposer就是一个Learner

- livelock

  - 多个Proposer并发对一个值运行Paxos时，可能会互相覆盖对方的rnd，然后提示自己的rnd再尝试，结果一直冲突，无法完成写入

#### Multi-paxos

- 整个系统**只有一个** Proposer，称之为 Leader

- 约为一轮RPC确定一个值（第一次RPC做了合并）
- 步骤
  - 若集群中没有 Leader，则在集群中选出一个节点并声明它为**第 M 任 Leader**
  - 集群的 Acceptor 只表决**最新的 Leader** 发出的最新的提案
- 优化：Multi-Paxos角色过多，对于计算机集群而言，可以将Proposer、Acceptor、Learner集中在一个节点上，这就是Raft

#### Fast Paxos

- 效果
  - 没冲突：1轮RPC确定一个值
  - 有冲突：2轮RPC确定一个值

- 过程
  - Proposer直接发送rnd=0的phrase-2请求，要成功写入quorum>n*3/4个Acceptor才算成功
  - Acceptor只在v=null时才接受phrase-2的请求
  - 如果发生冲突就回退到classic Paxos，发rnd>0的请求
- 如果像Classic Paxos那样quorum=n/2+1：
  - 假设Proposer X写入了3个Acceptor，Proposer Y写入了剩下的2个
  - 然后Y发现写入第3个时遇到冲突失败了，它已经访问过3=5/2+1个Acceptor了，但是它还是不知道哪个值才是应该被写入的，所以此时无法回退到Classic Paxos以修复冲突
- 解决办法：成功写入quorum>n*3/4个Acceptor才算成功
  - 这样，当另一个客户端进程访问了Classic Paxos quorum=n/2+1个Acceptor时，一定能够知道应该被写入的v

Raft

- 角色
  - Leader：负责发出提案
  - Follower：负责同意Leader发出的提案
  - Candidate：负责争夺Leader
- 步骤
  - Raft算法将一致性问题分为Leader选举、状态复制
  - Leader选举
    - 每个Follower都有一个定时器，如果时间到了还没有Leader，该Follower将声明自己是Leader参与Leader选举，同时将消息发给其他节点来争取它们的选票
    - 若其他节点长时间没有响应，节点将重新发起选举
    - 获得多数派支持的Candidate将成为第M任Leader（M是最新的任期）
    - 在任期内Leader将不断发送心跳给其他节点，其他节点回复心跳并清零自己的定时器，防止参选Leader
    - 若一定时间内Follower没有收到心跳，就参选Leader了
    - 如果两个Candidate收到的投票数相同，则重新选举
  - 状态复制
    - Leader负责接收来自于Client的提案请求
    - 提案内容将包含在Leader发出的下一个心跳中
    - Leader收到多数派Follower的回复后确认提案，写入自己的存储空间并回复Client
    - Leader通知Follower确认提案写入自己的存储空间，随后所有节点都拥有相同数据
    - 若集群中出现网络异常被分割，将出现多个Leader
    - 被分割出的非多数派将无法达成共识确认提案，即脑裂
    - 当集群再次连通时，将只听从最新Leader的指挥，旧Leader将退化为Follower

#### ZAB

- ZAB是对Multi-Paxos的改进，大部分和Raft相同，除了：
  - Leader的任期在Raft中叫term，在ZAB中叫epoch
  - 状态复制在Raft中是从Leader到Follower，在ZAB中相反

#### Gossip

- 节点间完全平等，每个节点都将数据改动告诉其他节点
- 步骤
  - 某个节点收到数据后将数据告诉其他四个节点
  - 收到数据改动的节点重复上面的过程直到所有的节点都被感染了































































