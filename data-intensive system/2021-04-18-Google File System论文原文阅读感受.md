对于File system，我其实没有那么熟悉，所以看GFS时感觉比看MR时吃力不少。看MR时，我感觉我已经弄懂了大部分细节，但是看GFS到现在，我还是感觉对于其中一部分内容不甚了解。看这篇文章时，我的一些感受如下：

- 听MIT课程的Robert Morris说，其实这个系统并没有什么新的东西，但是它在真正的超大集群上跑起来了，反映了实现上的技巧，是一个很好的工程实践
- 整个项目都是围绕着Google内部的workload展开的，在讨论一些可能的问题时，也都是从workload的角度出发的，包括对文件的随机写入几乎不存在、客户端不需要缓存、主要是处理大文件（通常100MB以上，也有数G的）。
  - workload有两种：大规模的流式读取和小规模的随机读取。流式读取通常是读取数百KB、数MB，小规模的随机读取则是在文件的某个随机的位置读取几个KB，所以可以把小规模的随机读取请求合并成顺序读取。
  - 主要追求吞吐量和网络带宽，而非低时延。
- 它引入了很多原子性的操作，保证了每个追加操作都是原子性的，从而保证了多个客户端能够同时进行追加操作，这个比较难以理解
- master只负责提供元信息，client获得了元信息后可以直接和chunk server交流传输。
- 客户端会缓存元信息。
- client会根据IP地址请求最近的副本。根据MIT课程的FAQ，2003年时，Google必定是根据服务器的实际地理位置来分配它们的IP地址空间。
- GFS可以预防脑裂问题。问题描述：如master认为某个chunk的primary server（如S1）挂了，但是S1其实没有挂，然后master分配S2为primary。预防方式：S1是primary身份是有lease（租期）的，如果过期了就得再向master申请。所以，即使master觉得S1挂了，也会等到租期结束然后把primary身份的lease转移给S2.
- 每个文件都会保存3个副本。
- 每个chunk 64MB，比普通文件系统的block大，同时采用惰性分配的策略，以减少空间浪费。选择这么大的chunk的原因是：减少master节点上的元信息大小、客户端可以缓存下元信息、客户端可以和chunk server保存长时间的传输，减少TCP握手。
- master不会持久化chunk server和chunk副本的映射关系，因为设计者发现在启动时轮询并且之后定期轮询 chunk 服务器会更简单，这样也可以灵活应对chunk服务器的新增、失效、硬盘损坏等情况。

- 使用快照+操作日志持久化来确保数据不容灾
- 对于一个包含数百万个文件的集群，创建快照需要1分钟左右的时间。
- 写入操作的语义是至少一次，所以需要客户端去重。
- 为了确保检测出硬盘磁道损坏、写入时等问题，写入时要加上checksum，读取时要验证checksum。 
- 写入采用了线性拓扑结构，即客户端只用写入离它最近的那个副本，然后那个chunk server再写入到其他chunk server。1MB数据在理想情况下80ms内就能分发出去。
- 每个chunk都按照每64KB分成一块，每个块都有checksum
- 在chunk服务器空闲时，它会扫描不活跃的chunk的内容，查看数据是否完整
- 实现时最大的问题是磁盘和Linux相关的问题，各种磁盘不兼容之类、系统调用效率的问题
- 据说，Google已经将GFS换成了Colossus，提高了master的性能和容错能力。
- 如果master的机器或磁盘挂了，处于GFS外部的监控进程就会某个拥有完整操作日志的机器上启动一个新的master进程。类似DNS别名，客户端使用master的名字来访问master节点，得到了一个新的地址，从而将master的名字和IP地址解耦。同时，GFS中有一些影子master服务器，在primary master服务器挂了时提供只读服务。它们是影子而非镜像，所以比master服务器慢1秒内。影子master服务器也会和chunk服务器进行各种交流。在主master服务器增删副本时，影子master才会和主master交流。
- 使用COW来实现快照生成。